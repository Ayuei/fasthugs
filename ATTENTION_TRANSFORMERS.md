A few super-helpful resources to better understand Attention and Transformers, in recommended order:

0. Luis Serano: [A Friendly Introduction to RNNs](https://www.youtube.com/watch?v=UNmqTiOnRfg)
1. Jay Alamar: [Seq2Seq RNN model with Attention](https://jalammar.github.io/illustrated-transformer/)
2. Jay Alamar: [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
3. Harvard NLP: [The Anotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
4. [@bentrevett's Seq2Seq explainer notebooks](https://github.com/bentrevett/pytorch-seq2seq)
5. [Annotated GPT-2](https://amaarora.github.io/2020/02/18/annotatedGPT2.html)
6. Andrew Peng: [Translation with transformer](https://andrewpeng.dev/transformer-pytorch/)
