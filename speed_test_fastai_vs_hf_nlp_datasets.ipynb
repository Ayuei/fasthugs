{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Speed: Fastai vs HuggingFace nlp Datasets\"\n",
    "\n",
    "> Speedtest: Fastai's `TextDataloders` vs HuggingFace's `nlp` Datasets\n",
    "\n",
    "- badges: true\n",
    "- categories: [nlp, fastai, dataloader]\n",
    "- image: images/bokeh_mini.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs:\n",
    "Add post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tl;dr\n",
    "gggg\n",
    "\n",
    "## Speed\n",
    "I started playing around with the `nlp` library recently and was blown away by the speed at which you can iterate through the data (thanks to PyArrow wizardry), its seriously fast!\n",
    "\n",
    "> twitter: https://twitter.com/Thom_Wolf/status/1272512974935203841\n",
    "\n",
    "So I wondered if there was a significant speed up to be gained by doing as much data processing as I could with the library. After previously discovering Fastai's funcionality to do [faster text loading](https://forums.fast.ai/t/nlp-speed-up-if-using-sorteddl/74636) I was in the market for more speed!\n",
    "\n",
    "The library supports not only 100+ common datasets but thanks to [this pointer from Thomas Wolf](https://discuss.huggingface.co/t/nlp-0-3-0-is-out/50/3) on the new HuggingFace forums I learned that you can also easily load your own csvs and bask in all of that speedy goodness!\n",
    "\n",
    "```\n",
    "from nlp import load_dataset\n",
    "\n",
    "dataset = load_dataset('csv', data_files='my_file.csv')\n",
    "dataset = load_dataset('csv', data_files=['my_file_1.csv', 'my_file_2.csv', 'my_file_3.csv'])\n",
    "dataset = load_dataset('csv', data_files={'train': ['my_train_file_1.csv', 'my_train_file_2.csv'], \n",
    "                                          'test': 'my_test_file.csv'})\n",
    "\n",
    "```\n",
    "\n",
    "So, is it faster?\n",
    "\n",
    "## Experiment setup\n",
    "\n",
    "We'll be comparing Fastai's high-level `TextDataloders` class to a custom dataprocessing pipeline using HuggingFace's `nlp` datasets library.\n",
    "\n",
    "This Fastai class does a bunch of different things:\n",
    "- Pre and Post Processing\n",
    "- Tokenization: The default uses Spacy's tokenizer and creates a vocabulary and parallelises the tokenization\n",
    "- Optimizations: Sorting data by text sample length and padding only to the longest item in the sequence, [similar what was described here](https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e)\n",
    "- Creates train and validation dataloaders\n",
    "\n",
    "The `nlp` Datasets pipeline I wrote tries to replicate all of the core functionality of `TextDataloaders` as best I could. \n",
    "\n",
    "> Note: I couldn't figure out how to parallelise the text processing with `nlp` although this is probably down to my lack of experience with parallelism as opposed to a limitation of the library\n",
    "\n",
    "### Sentiment Dataset\n",
    "For this experiment I used the [Sentiment140](https://huggingface.co/datasets/sentiment140) dataset, a sentiment classifcation dataset of Twitter data. \n",
    "\n",
    "For our experiment we'll use\n",
    "- 10% of sentiment dataset (160,000 tweets, 11.8M space-separated tokens), pulled from nlp library\n",
    "- 80/20 train/val split\n",
    "\n",
    "### Experiment Settings\n",
    "A full timed run comprises of:\n",
    "\n",
    "0. Reading the data from disk, from a csv for fastai and from a PyArrow file for `nlp`\n",
    "\n",
    "1. Applying [fastai's default text pre-processing functions](http://dev.fast.ai/text.core#Preprocessing-rules). These will:\n",
    "\n",
    "\n",
    "    Fix various messy bits of html sometimes seen in documents\n",
    "    Replace repetitions at the character level, e.g. `cccc` becomes: `TK_REP 4 c`\n",
    "    Replace word repetitions, e.g. `cow cow cow cow` becomes: `TK_WREP 4 cow`\n",
    "    Add spaces around / and #\n",
    "    Remove multiple spaces \n",
    "    Replace tokens in ALL CAPS by their lower version and add TK_UP before.\n",
    "    Replace characters in ALL CAPS by their lower version and add TK_UP before.\n",
    "    Lowercases everything\n",
    "\n",
    "\n",
    "2. Tokenizing based on Spacy's tokenizer (fastai's default)\n",
    "\n",
    "3. Applying a post-processing rule which replaces embedded spaces in a token with unicode line char to allow for split/join\n",
    "\n",
    "4. Performing 1 epoch iterating through the training data, bs = 64\n",
    "\n",
    "\n",
    "\n",
    "## Results\n",
    "\n",
    "#### 10% Data\n",
    "Results are...mixed! While the Fastai convienience function had a faster init (48s vs 71s), the PyArrow-backed `nlp` run through a single epoch was significantly faster (11s vs 14s).\n",
    "\n",
    "| 0.16M ROWS: | Init (s)| 1 epoch (s) | 1 mini-batch [bs=64] (ms) | 1.6M ROWS: | Init (s) | 1 epoch (s) |\n",
    "| :- | :-: | :-: | :-: | :-: | :-: | :-: |\n",
    "| **Fastai** | 124 | 14.28 | 7.4 | - | | |\n",
    "| **Fastai w/sorted** | **48.1** | 14.25 | 7.4 | - | | |\n",
    "| **nlp** | 71.2 | **11.27** | 5.6 | - | 1290 | |\n",
    "\n",
    "#### 100% Data\n",
    "With 100% of the data, the difference in init time is clearer. \n",
    "\n",
    "| 1.6M ROWS: | Init (s) | 1 epoch (s) |\n",
    "| :- | :-: | :-: |\n",
    "| **Fastai** | | |\n",
    "| **Fastai w/sorted** | | |\n",
    "| **nlp**| 1290 | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def timings(n_epochs, init, per_ep):\n",
    "    #init=48\n",
    "    #eps = n_epochs * 14.25\n",
    "    eps = n_epochs * per_ep\n",
    "    return init+eps\n",
    "\n",
    "fastai_sorted_10 = [48, 14.25]\n",
    "nlp_10 = [71, 11.27]\n",
    "\n",
    "timings_ls = []\n",
    "timings = [fastai_sorted_10, nlp_10]\n",
    "\n",
    "n_eps = list(range(0,20,2))\n",
    "\n",
    "[[timings(n_epochs=n, init=t[0], per_ep=t[1]) for n in n_eps] for t in timings]\n",
    "\n",
    "# def nlp_sorted(n_epochs):\n",
    "#     init=71\n",
    "#     eps = n_epochs * 11.27\n",
    "#     return init+eps\n",
    "\n",
    "# n_eps = list(range(0,20,2))\n",
    "# f_sorted, nlp_ls =[],[]\n",
    "# for n in n_eps:\n",
    "#     f_sorted.append(fastai_sorted(n))\n",
    "#     nlp_ls.append(nlp_sorted(n))\n",
    "    \n",
    "plt.plot(f_sorted)\n",
    "plt.plot(nlp_ls);\n",
    "\n",
    "for timing_data in timings:\n",
    "    plt.plot(timing_data)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from fastai2.basics import *\n",
    "from fastai2.text.all import *\n",
    "# from fastai2.callback.all import *\n",
    "# from fastai2.data.transforms import RandomSplitter\n",
    "from fastai2.text.core import defaults\n",
    "\n",
    "from nlp import load_dataset\n",
    "\n",
    "import spacy,html\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, Dynamic Padding is only needed if actually feeding batches...if not them SortedDL should also be bypassed!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<function fix_html at 0x7f25ff011ef0>, <function replace_rep at 0x7f25ff011dd0>, <function replace_wrep at 0x7f25ff011e60>, <function spec_add_spaces at 0x7f26106f75f0>, <function rm_useless_spaces at 0x7f261067bb00>, <function replace_all_caps at 0x7f25ff011f80>, <function replace_maj at 0x7f25ff01c050>, <function lowercase at 0x7f25ff01c0e0>] \n",
      "\n",
      " [<function replace_space at 0x7f25ff01c170>] \n",
      "\n",
      " ['xxunk', 'xxpad', 'xxbos', 'xxeos', 'xxfld', 'xxrep', 'xxwrep', 'xxup', 'xxmaj']\n"
     ]
    }
   ],
   "source": [
    "#hide \n",
    "# The Pre and Post-Processing functions as well as the special tokens can be found here\n",
    "print(defaults.text_proc_rules,'\\n\\n', defaults.text_postproc_rules,'\\n\\n', defaults.text_spec_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fastai Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_collapse\n",
    "#%%timeit\n",
    "\n",
    "# Read data; the first 10% of the sentiment140 dataset, extraced from the `nlp` library and saved as a csv\n",
    "fn_10pct = 'sentiment140_10pct.csv'\n",
    "df = pd.read_csv(fn_10pct, index_col=None)\n",
    "\n",
    "# SORT: Calculate text sample lengths\n",
    "df['word_count'] = df['text'].str.split().map(len)\n",
    "\n",
    "res=df['word_count'].values\n",
    "\n",
    "# Create Dataloaders\n",
    "dls = TextDataLoaders.from_csv(path='.', csv_fname=fn_10pct, valid_pct=0.2, bs=64, \n",
    "                               text_col='text', label_col='sentiment' , res=res)\n",
    "\n",
    "# Do 1 pass of the training dataloader\n",
    "s = \"\"\"for b in dls.train:\n",
    "            pass\n",
    "    \"\"\"\n",
    "\n",
    "time = timeit.timeit(stmt=s, number=1, globals=globals()); time\n",
    "time, time / len(dls.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace `nlp` Datasets Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer, Numericalizer and Padding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_collapse\n",
    "class SpacyTokenizerNLP():\n",
    "    \"Spacy tokenizer for `lang`\"\n",
    "    def __init__(self, lang='en', special_toks=None, buf_sz=5000):\n",
    "        self.special_toks = ifnone(special_toks, defaults.text_spec_tok)\n",
    "        nlp = spacy.blank(lang, disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "        for w in self.special_toks: nlp.tokenizer.add_special_case(w, [{ORTH: w}])\n",
    "        self.pipe,self.buf_sz = nlp.pipe,buf_sz\n",
    "        \n",
    "    def encodes(self, items):\n",
    "        tmp = [list(doc) for doc in self.pipe(items, batch_size=self.buf_sz)]\n",
    "        return {'tok_text_pre': [list(str(t) for t in l) for l in tmp]}\n",
    "\n",
    "def make_vocab(count, min_freq=3, max_vocab=60000, special_toks=None):\n",
    "    \"Create a vocab of `max_vocab` size from `Counter` `count` with items present more than `min_freq`\"\n",
    "    vocab = [o for o,c in count.most_common(max_vocab) if c >= min_freq]\n",
    "    special_toks = ifnone(special_toks, defaults.text_spec_tok)\n",
    "    for o in reversed(special_toks): #Make sure all special tokens are in the vocab\n",
    "        if o in vocab: vocab.remove(o)\n",
    "        vocab.insert(0, o)\n",
    "    vocab = vocab[:max_vocab]\n",
    "    return vocab + [f'xxfake' for i in range(0, 8-len(vocab)%8)]\n",
    "\n",
    "class NumericalizeNLP(Transform):\n",
    "    \"Reversible transform of tokenized texts to numericalized ids\"\n",
    "    def __init__(self, dsets=None, vocab=None, min_freq=3, max_vocab=60000, special_toks=None, pad_tok=None):\n",
    "        store_attr(self, 'vocab,min_freq,max_vocab,special_toks,pad_tok')\n",
    "        self.vocab, self.special_toks, self.min_freq, self.max_vocab = vocab, special_toks, min_freq, max_vocab\n",
    "        self.o2i = None if vocab is None else defaultdict(int, {v:k for k,v in enumerate(vocab)})\n",
    "\n",
    "        if self.vocab is None:\n",
    "            count = Counter(p for o in dsets for p in o)\n",
    "            self.vocab = make_vocab(count, min_freq=self.min_freq, max_vocab=self.max_vocab, special_toks=self.special_toks)\n",
    "            self.o2i = defaultdict(int, {v:k for k,v in enumerate(self.vocab) if v != 'xxfake'})\n",
    "    \n",
    "    def encodes_nlp(self, o): return TensorText(tensor([self.o2i  [o_] for o_ in o]))\n",
    "    def encodes_nlp(self, b): return {'toks' : [[self.o2i[o_] for o_ in oo] for oo in b['tok_text']]}\n",
    "    \n",
    "# Padding functions\n",
    "def pad_seq(x, max_batch_len, pad_idx):    \n",
    "    pad =  x.new_zeros(max_batch_len-x.size(0))+pad_idx\n",
    "    return torch.cat([x, pad])\n",
    " \n",
    "# Pad up to longest item in the batch and put batch on the GPU\n",
    "def pad_batch(batch=None, pad_token_id=1):\n",
    "    batch_inputs = list()\n",
    "    max_size = max([len(item['toks']) for item in batch])\n",
    "    for item in batch:\n",
    "        batch_inputs += [pad_seq(item['toks'], max_size, pad_token_id)]\n",
    "    return torch.stack(batch_inputs).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset sentiment140/sentiment140 (download: 77.59 MiB, generated: 214.21 MiB, total: 291.81 MiB) to /home/morgan/.cache/huggingface/datasets/sentiment140/sentiment140/1.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sentiment140 downloaded and prepared to /home/morgan/.cache/huggingface/datasets/sentiment140/sentiment140/1.0.0. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "#hide_collapse\n",
    "\n",
    "# Download text, a clean version of the dataset is downloaded (not included in the timings)\n",
    "senti_dataset = load_dataset('sentiment140', split='train[:100%]', download_mode='reuse_cache_if_exists')\n",
    "\n",
    "spacy_tok = SpacyTokenizerNLP(lang='en', special_toks=defaults.text_spec_tok)\n",
    "\n",
    "def preproc_and_tok(b): return spacy_tok.encodes(list(maps(*defaults.text_proc_rules, b['text'])))\n",
    "\n",
    "def postproc(b): \n",
    "    return {'tok_text': [list(maps(*defaults.text_postproc_rules, _b)) for _b in b['tok_text_pre']]}\n",
    "\n",
    "def get_tok_lengths(example_batch): return {'tok_lens': [len(e) for e in example_batch['toks']]}\n",
    "\n",
    "def prepare_dataset(dataset):\n",
    "    '''\n",
    "        Takes a raw nlp dataset and returns a processed, tokenized, numericalised dataset\n",
    "    '''\n",
    "    # Apply processing rules and tokenize\n",
    "    dataset = dataset.map(preproc_and_tok, batched=True)\n",
    "\n",
    "    # Apply post-processing rules \n",
    "    dataset = dataset.map(postproc, batched=True)\n",
    "\n",
    "    # Init Numericalizer and create vocab\n",
    "    numeric = NumericalizeNLP(dsets=dataset['tok_text_pre'], special_toks=defaults.text_spec_tok, pad_tok=1)\n",
    "\n",
    "    # Numericalize\n",
    "    dataset = dataset.map(numeric.encodes_nlp, batched=True)\n",
    "\n",
    "    # Get sample lengths for sorting\n",
    "    dataset=dataset.map(get_tok_lengths, batched=True)\n",
    "\n",
    "    # Sort dataset from small to large\n",
    "    dataset = dataset.sort('tok_lens')\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [06:53<00:00,  3.87it/s]\n",
      "100%|██████████| 1600/1600 [00:27<00:00, 58.24it/s]\n",
      "100%|██████████| 1600/1600 [00:32<00:00, 49.38it/s]\n",
      "100%|██████████| 1600/1600 [00:33<00:00, 47.25it/s]\n",
      "100%|██████████| 1600000/1600000 [06:48<00:00, 3912.30it/s]\n",
      " 69%|██████▉   | 887967/1280000 [02:24<01:50, 3546.51it/s] "
     ]
    }
   ],
   "source": [
    "#%%timeit -n 1 -r 1\n",
    "#hide_collapse\n",
    "\n",
    "# Do all of the text processing, tokenization and numericalization\n",
    "senti_dataset_f = prepare_dataset(senti_dataset)\n",
    "\n",
    "# Create train and test splits: `.train_test_split` is giving me an error, lets use `.select` instead\n",
    "train_split=int(len(senti_dataset_f)*0.8)\n",
    "train_senti = senti_dataset_f.select(list(range(train_split)))\n",
    "test_senti = senti_dataset_f.select(list(range(train_split, len(senti_dataset_f))))\n",
    "\n",
    "# Format our dataset to outputs torch.Tensor to train a pytorch model\n",
    "columns = ['toks']\n",
    "train_senti.set_format(type='torch', columns=columns)\n",
    "test_senti.set_format(type='torch', columns=columns)\n",
    "\n",
    "# Instantiate out PyTorch Dataloaders \n",
    "train_dataloader = torch.utils.data.DataLoader(train_senti, batch_size=64, collate_fn=pad_batch)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_senti, batch_size=64, collate_fn=pad_batch)\n",
    "\n",
    "# # Do 1 epoch\n",
    "# for b in train_dataloader: \n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "s = \"\"\"for b in train_dataloader: \n",
    "            pass\n",
    "    \"\"\"\n",
    "time = timeit.timeit(stmt=s, number=1, globals=globals()); time\n",
    "time, time / (len(train_senti)/64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
