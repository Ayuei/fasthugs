{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastHugs\n",
    "This notebook gives a full run through to fine-tune a text classification model with **HuggingFace transformers** and the new **fastai-v2** library.\n",
    "\n",
    "## Things You Might Like\n",
    "**FastHugsTokenizer:** A tokenizer wrapper than can be used with fastai-v2's tokenizer.\n",
    "\n",
    "**FastHugsModel:** A model wrapper over the HF models, more or less the same to the wrapper's from HF fastai-v1 articles mentioned below\n",
    "\n",
    "**Vocab:** A function to extract the vocab depending on the pre-trained transformer (HF hasn't standardised this processes ðŸ˜¢ ).\n",
    "\n",
    "**Padding:** Padding settings for the padding token index and on whether the transformer prefers left or right padding\n",
    "\n",
    "**Vocab for Albert-base-v2**: .json for Albert-base-v2's vocab, otherwise this has to be extracted from a SentencePiece model file, which isn't fun\n",
    "\n",
    "\n",
    "### Pretrained Transformers only for now\n",
    "Initially, this notebook will only deal with finetuning HuggingFace's pretrained models. It covers BERT, DistilBERT, RoBERTa and ALBERT pretrained classification models only. These are the core transformer model architectures where HuggingFace have added a classification head. HuggingFace also has other versions of these model architectures such as the core model architecture and language model model architectures.\n",
    "\n",
    "If you'd like to try train a model from scratch HuggingFace just recently published an article on [How to train a new language model from scratch using Transformers and Tokenizers](https://huggingface.co/blog/how-to-train). Its well worth reading to see how their `tokenizers` library can be used, independent of their pretrained transformer models.\n",
    "\n",
    "### Read these first ðŸ‘‡\n",
    "This notebooks heavily borrows from [this notebook](https://www.kaggle.com/melissarajaram/roberta-fastai-huggingface-transformers) , which in turn is based off of this [tutorial](https://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta) and accompanying [article](https://towardsdatascience.com/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2). Huge thanks to  Melissa Rajaram and Maximilien Roberti for these great resources, if you're not familiar with the HuggingFace library please \n",
    "\n",
    "### fastai-v2\n",
    "[This paper](https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/) introduces the v2 version of the fastai library and you can follow and contribute to v2's progress [on the forums](https://forums.fast.ai/). This notebook is based off the [fastai-v2 ULMFiT tutorial](http://dev.fast.ai/tutorial.ulmfit). Huge thanks to Jeremy, Sylvain, Rachel and the fastai community for making this library what it is. I'm super excited about the additinal flexibility v2 brings.\n",
    "\n",
    "### Dependencies\n",
    "If you haven't already, install HuggingFace's `transformers` library with: `pip install transformers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Vocab\n",
    " Model and vocab files will be saved with files names as a long string of digits and letters (e.g. `d9fc1956a0....f4cfdb5feda.json` generated from the etag from the AWS S3 bucket as described [here in the HuggingFace repo](https://github.com/huggingface/transformers/issues/2157). For readability I prefer to save the files in a specified directory and model name so that it can be easily found and accessed in future.\n",
    " \n",
    "(Note: To avoid saving these files twice you could look at the `from_pretrained` and `cached_path` functions in HuggingFace's `PreTrainedTokenizer` class definition to find the code that downloads the files and maybe modify them to download directly to your specified directory withe desired name. I haven't had time to go that deep.)\n",
    "\n",
    "Load vocab file into a `list` as expected by fastai-v2. The HF pretrained tokenizer vocabs come in different file formats depending on the tokenizer you're using; BERT's vocab is saved as a .txt file, RoBERTa's is saved as a .json and Albert's has to be extracted from a SentencePiece model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(transformer_tokenizer, pretrained_model_name):\n",
    "    if pretrained_model_name in ['bert-base-uncased', 'distilbert-base-uncased']:\n",
    "        transformer_vocab = list(transformer_tokenizer.vocab.keys())\n",
    "    else:\n",
    "        transformer_tokenizer.save_vocabulary(model_path/f'{pretrained_model_name}')\n",
    "        suff = 'json'\n",
    "        if pretrained_model_name in ['albert-base-v2']:\n",
    "            with open(model_path/f'{pretrained_model_name}/alberta_v2_vocab.{suff}', 'r') as f: \n",
    "                transformer_vocab = json.load(f) \n",
    "        else:\n",
    "            with open(model_path/f'{pretrained_model_name}/vocab.{suff}', 'r') as f: \n",
    "                transformer_vocab = list(json.load(f).keys()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_vocab = get_vocab(transformer_tokenizer, pretrained_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Albert Vocab\n",
    "### From SentencePiece issues: https://github.com/google/sentencepiece/issues/121\n",
    "\n",
    "**1. Install protobuf**\n",
    "\n",
    "`sudo apt install protobuf-compiler`\n",
    "\n",
    "\n",
    "\n",
    "**2. Clone the SentencePiece repo**\n",
    "\n",
    "`git clone https://github.com/google/sentencepiece.git`\n",
    "\n",
    "\n",
    "\n",
    "**3. cd to `sentencepiece/src`**\n",
    "\n",
    "`cd sentencepiece/src`\n",
    "\n",
    "\n",
    "**4. Run the below to generate `sentencepiece_model_pb2.py`**\n",
    "\n",
    "`protoc --python_out=. sentencepiece_model.proto`\n",
    "\n",
    "\n",
    "**5. Copy the `sentencepiece_model_pb2.py` file to the same directory as your notebook**\n",
    "\n",
    "`cp sentencepiece_model_pb2.py YOUR_NOTEBOOK_DIR/sentencepiece_model_pb2.py`\n",
    "\n",
    "\n",
    "**6. Use `sentencepiece_model_pb2` to open the .model file from the tokenizer**\n",
    "\n",
    "`\n",
    "import sentencepiece_model_pb2 as spmodel\n",
    "m = spmodel.ModelProto()\n",
    "m.ParseFromString(open('models/spiece.model', 'rb').read())\n",
    "`\n",
    "\n",
    "**7. Iterate through .pieces to extract each token from the vocab and append to list**\n",
    "`\n",
    "vocab_ls=[]\n",
    "for i,p in enumerate(m.pieces):\n",
    "    vocab_ls.append(p.piece)\n",
    "`\n",
    "\n",
    "(There is also a p.score attribute if you are interested in that too)\n",
    "\n",
    "\n",
    "**8. Save the vocab so you don't have to do this icky work again**\n",
    "\n",
    "`\n",
    "import json\n",
    "with open('YOUR_MODEL_DIR/alberta_v2_vocab.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(vocab_ls, f, ensure_ascii=False, indent=4)\n",
    " `\n",
    " \n",
    "**Opening the vocab.json file in future**\n",
    "\n",
    "Now simply use the below code to open your saved vocab file\n",
    "\n",
    "`\n",
    "with open('models/albert-base-v2/alberta_v2_vocab.txt', 'w') as f:\n",
    "    for item in v_ls:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
